# **[CNS 2018 workshop](http://www.cnsorg.org/cns-2018)**
Large neural networks, biological or artificial, can learn complex input-output relations. During learning, the network dynamics are often constrained to a low-dimensional manifold despite available high-dimensional space. The mechanism behind this space dimensionality confinement is yet unclear. 

Current technological advances in chronic population recordings and optogenetics provide the tools to measure and manipulate the reorganization of this state-space structure in neural circuits in awake, behaving animals during learning. 

We will bring together theoreticians and experimentalists to address a most fundamental question in neuroscience, that is, how learning reshapes collective network activity. 

More specifically, we will explore the following questions: 
* How does the neural dimensionality of a learned task relate to the task complexity? 
* Which mathematical tools are suitable to identify low-dimensional neural manifolds and track their emergence during learning?
* How does the dimensionality constrain the learning capabilities?

### [Registration](https://ocns.memberclicks.net/cns-2018 "CNS 2018 registration")
Note that the early registration deadline for the Seattle meeting (including workshops) is now very close: May 7 for non-members of OCNS, and May 16 for members.

### Speakers: 
* [SueYeon Chung](https://sites.google.com/site/sueyeonchung/) (Harvard University):  
*Classification and geometry of neural manifolds, and the application to deep networks*
* [Rainer Engelken](https://ctn.zuckermaninstitute.columbia.edu/people/rainer-engelken) (Columbia University):  
*Dimensionality and entropy rate of spontaneous and evoked neural rate dynamics*
* [Kameron Decker Harris](http://faculty.washington.edu/kamdh/) (University of Washington):  
*Connections between dimensionality and network sparsity*
* [Zack Kilpatrick](https://www.colorado.edu/amath/zpkilpat) (University of Colorado Boulder):  
*Learning continuous attractors in recurrent neural networks*
* [Guillaume Lajoie](https://dms.umontreal.ca/~lajoie/) (Université de Montréal):  
*External perturbations modulate coding manifolds and dimensionality of motor cortex activity*
* [Luca Mazzucato](https://lucamazzucato.weebly.com/) (Columbia University, University of Oregon):  
*Changes in effective network coupling mediate learning in a trace fear conditioning task*
* [Stefano Recanatesi](https://faculty.washington.edu/etsb/) (University of Washington):  
*Explaining the dimensionality of the activity in RNNs through connectivity motifs*
* [Merav Stern](https://faculty.washington.edu/ms4325/) (University of Washington):  
*Increased correlations and decreased activity dimensions during task performance*
* [Evelyn Tang](https://scholar.google.com/citations?user=CQbaZpMAAAAJ&hl=en) (University of Pennsylvania):  
*Effective learning is accompanied by high dimensional and efficient representations of neural activity*
* [Alex Williams](http://alexhwilliams.info/) (Stanford University):  
*Dimensionality reduction with single trial resolution*


### Schedule: 
coming soon

### Organizers

* Rainer Engelken, Center for Theoretical Neuroscience, Columbia University
* Guillaume Lajoie, Dept. de Mathématiques et Statistiques, Université de Montréal
* Merav Stern, Department of Applied Mathematics, University of Washington

![visualization of low-dimensional attractor of chaotic firing-rate network by Rainer Engelken](http://www.columbia.edu/~re2365/attractor.png)
