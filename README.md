Large neural networks, biological or artificial, can learn complex input-output relations. During learning, the network dynamics are often constrained to a low-dimensional manifold despite available high-dimensional space. The mechanism behind this space dimensionality confinement is yet unclear. 

Current technological advances in chronic population recordings and optogenetics provide the tools to measure and manipulate the reorganization of this state-space structure in neural circuits in awake, behaving animals during learning. 

We will bring together theoreticians and experimentalists to address a most fundamental question in neuroscience, that is, how learning reshapes collective network activity. 

More specifically, we will explore the following questions: 
* How does the neural dimensionality of a learned task relate to the task complexity? 
* Which mathematical tools are suitable to identify low-dimensional neural manifolds and track their emergence during learning?
* How does the dimensionality constrain the learning capabilities?

### [Registration](https://ocns.memberclicks.net/cns-2018 "CNS 2018 registration")
Note that the early registration deadline for the Seattle meeting (including workshops) is now very close: May 7 for non-members of OCNS, and May 16 for members.

### Speakers: 
* SueYeon Chung (Harvard University)
* Rainer Engelken (Columbia University)
* Ila Fiete  (UT Austin) (*tbc*)
* Surya Ganguli (Stanford University) (*tbc*)
* Guillaume Lajoie (Université de Montréal)
* Luca Mazzucato (Columbia University, University of Oregon)
* Stefano Recanatesi (University of Washington)
* Merav Stern (University of Washington)
* Evelyn Tang (University of Pennsylvania)
* Zack Kilpatrick (University of Colorado Boulder)


### Schedule: 
coming soon

### Organizers

* Rainer Engelken, Center for Theoretical Neuroscience, Columbia University
* Guillaume Lajoie, Dept. de Mathématiques et Statistiques, Université de Montréal
* Merav Stern, Department of Applied Mathematics, University of Washington

